'use client'

import React, { useState, useRef } from 'react'
import Button from "@/components/ui/Button"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"

interface DialogueSimulationProps {
  scenario: string;
  onComplete: (feedback: string) => void;
}

interface Message {
  role: 'user' | 'assistant';
  content: string;
  audioUrl?: string;
}

export function DialogueSimulation({ scenario, onComplete }: DialogueSimulationProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [feedback, setFeedback] = useState<string>('');
  const [conversation, setConversation] = useState<Message[]>([]);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);

  const startSimulation = async () => {
    setConversation([]);
    
    // å¼€å§‹åœºæ™¯å¯¹è¯
    const initialMessage = `Let's practice a ${scenario} scenario. I'll be the ${
      scenario === 'restaurant' ? 'waiter' : 
      scenario === 'airport' ? 'check-in staff' :
      scenario === 'hotel' ? 'receptionist' : 'staff'
    }. How can I help you?`;

    // æ·»åŠ å¹¶æ’­æ”¾åˆå§‹æ¶ˆæ¯
    await addAssistantMessage(initialMessage);
    
    // è®¾ç½®å½•éŸ³çŠ¶æ€ä¸º trueï¼Œå…è®¸ç”¨æˆ·å¼€å§‹å½•éŸ³
    setIsRecording(true);
  };

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/wav' });
        const audioUrl = URL.createObjectURL(audioBlob);
        
        // æ·»åŠ ç”¨æˆ·çš„è¯­éŸ³æ¶ˆæ¯
        setConversation(prev => [...prev, {
          role: 'user',
          content: 'æ­£åœ¨å¤„ç†è¯­éŸ³...',
          audioUrl
        }]);

        await processUserResponse(audioBlob);
      };

      mediaRecorder.start();
      setIsRecording(true);
    } catch (error) {
      console.error('Error accessing microphone:', error);
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    setIsRecording(false);
  };

  const processUserResponse = async (audioBlob: Blob) => {
    try {
      const transcriptionFormData = new FormData();
      transcriptionFormData.append('file', audioBlob, 'audio.wav');
      transcriptionFormData.append('model', 'whisper-1');
      transcriptionFormData.append('language', 'en');

      const transcriptionResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.NEXT_PUBLIC_OPENAI_API_KEY}`,
        },
        body: transcriptionFormData
      });

      const transcriptionData = await transcriptionResponse.json();
      const userMessage = transcriptionData.text;

      // æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯ï¼Œä¿ç•™éŸ³é¢‘URL
      setConversation(prev => [
        ...prev.slice(0, -1),
        {
          role: 'user',
          content: userMessage,
          audioUrl: prev[prev.length - 1].audioUrl
        }
      ]);

      // 2. è·å–åŠ©æ‰‹å›å¤
      const chatResponse = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.NEXT_PUBLIC_OPENAI_API_KEY}`,
        },
        body: JSON.stringify({
          model: 'gpt-4',
          messages: [
            {
              role: 'system',
              content: `ä½ æ˜¯ä¸€ä¸ªåœ¨${scenario}åœºæ™¯ä¸­çš„æœåŠ¡äººå‘˜ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„å›ç­”ç»§ç»­å¯¹è¯ã€‚æ¯æ¬¡å›å¤è¦ç®€çŸ­ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·ç»§ç»­å¯¹è¯ã€‚å½“åœºæ™¯å¯¹è¯è‡ªç„¶ç»“æŸæ—¶ï¼Œè¯´"Thank you for practicing with me."ã€‚`
            },
            ...conversation,
            { role: 'user', content: userMessage }
          ]
        })
      });

      const chatData = await chatResponse.json();
      const assistantMessage = chatData.choices[0].message.content;

      // å¦‚æœå¯¹è¯ç»“æŸï¼Œç»“æŸæ¨¡æ‹Ÿ
      if (assistantMessage.includes('Thank you for practicing with me')) {
        await addAssistantMessage(assistantMessage);
        endSimulation();
        return;
      }

      // æ·»åŠ å¹¶æ’­æ”¾åŠ©æ‰‹æ¶ˆæ¯
      await addAssistantMessage(assistantMessage);

    } catch (error) {
      console.error('Error processing response:', error);
    }
  };

  const addAssistantMessage = async (message: string) => {
    // æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å¯¹è¯
    setConversation(prev => [...prev, { role: 'assistant', content: message }]);

    // è½¬æ¢ä¸ºè¯­éŸ³å¹¶æ’­æ”¾
    try {
      const speechResponse = await fetch('https://api.openai.com/v1/audio/speech', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.NEXT_PUBLIC_OPENAI_API_KEY}`,
        },
        body: JSON.stringify({
          model: 'tts-1',
          input: message,
          voice: 'alloy',
          speed: 0.9
        })
      });

      const audioBlob = await speechResponse.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      await audio.play();
    } catch (error) {
      console.error('Error playing audio:', error);
    }
  };

  const endSimulation = async () => {
    setIsRecording(false);

    // è·å–è¯„ä¼°åé¦ˆ
    try {
      const feedbackResponse = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.NEXT_PUBLIC_OPENAI_API_KEY}`,
        },
        body: JSON.stringify({
          model: 'gpt-4',
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªè‹±è¯­å£è¯­æ•™ç»ƒã€‚è¯·ç”¨ä¸­æ–‡å¯¹å­¦ç”Ÿçš„å£è¡¨ç°è¿›è¡Œè¯„ä¼°ï¼ŒæŒ‡å‡ºä¼˜ç‚¹å’Œéœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚'
            },
            {
              role: 'user',
              content: `åœºæ™¯ï¼š${scenario}\nå¯¹è¯è®°å½•ï¼š${JSON.stringify(conversation)}`
            }
          ]
        })
      });

      const feedbackData = await feedbackResponse.json();
      const feedbackText = feedbackData.choices[0].message.content;
      setFeedback(feedbackText);
      onComplete(feedbackText);
    } catch (error) {
      console.error('Error getting feedback:', error);
    }
  };

  return (
    <Card className="max-w-2xl mx-auto">
      <CardHeader>
        <CardTitle className="flex items-center justify-between">
          <span>åœºæ™¯æ¨¡æ‹Ÿå¯¹è¯</span>
          {isRecording && (
            <span className="text-sm text-red-500 animate-pulse">â— å½•éŸ³ä¸­</span>
          )}
        </CardTitle>
      </CardHeader>
      <CardContent className="space-y-4">
        <div className="flex justify-center gap-4">
          {!conversation.length ? (
            <Button onClick={startSimulation}>
              å¼€å§‹å¯¹è¯
            </Button>
          ) : !isRecording ? (
            <Button onClick={startRecording}>
              å¼€å§‹å›ç­”
            </Button>
          ) : (
            <div className="flex gap-2">
              <Button 
                variant="destructive"
                onClick={stopRecording}
                className="animate-pulse"
              >
                ç»“æŸå½•éŸ³
              </Button>
            </div>
          )}
        </div>

        <div className="h-[400px] overflow-y-auto space-y-4 p-4 border rounded-lg">
          {conversation.length === 0 ? (
            <div className="text-center text-muted-foreground">
              ç‚¹å‡»"å¼€å§‹å¯¹è¯"å¼€å§‹ç»ƒä¹ 
            </div>
          ) : (
            conversation.map((message, index) => (
              <div
                key={index}
                className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}
              >
                <div
                  className={`max-w-[80%] rounded-lg p-3 ${
                    message.role === 'user'
                      ? 'bg-primary text-primary-foreground'
                      : 'bg-muted'
                  }`}
                >
                  <div className="text-sm font-medium mb-1 flex items-center justify-between">
                    <span>{message.role === 'user' ? 'ä½ ' : 'åŠ©æ‰‹'}</span>
                    {message.audioUrl && (
                      <Button
                        variant="ghost"
                        size="sm"
                        onClick={() => new Audio(message.audioUrl).play()}
                        className="ml-2 p-1 h-6"
                      >
                        ğŸ”Š
                      </Button>
                    )}
                  </div>
                  <div>{message.content}</div>
                </div>
              </div>
            ))
          )}
        </div>

        {feedback && (
          <div className="mt-6 p-4 bg-muted rounded-lg">
            <h3 className="font-semibold mb-2">è¯„ä¼°åé¦ˆï¼š</h3>
            <p className="text-muted-foreground">{feedback}</p>
          </div>
        )}
      </CardContent>
    </Card>
  );
} 